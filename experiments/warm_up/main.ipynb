{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from utils import get_cifar10_loaders, train_and_test, plot_pairwise, eval_on_dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "from resnet import resnet18\n",
    "from neural_collapse import NC\n",
    "import copy\n",
    "from tqdm import tqdm, trange\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "max_epoch_warm_up = 350\n",
    "log_interval = 5\n",
    "batch_size = 128\n",
    "lr = 0.001\n",
    "stop_acc = 0.99\n",
    "max_epoch_full = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, train_loader, criterion, optimizer):\n",
    "    '''\n",
    "    : param model: torch.nn.Module\n",
    "    : param train_loader: torch.utils.data.DataLoader\n",
    "    : param criterion: torch.nn.Module\n",
    "    : param optimizer: torch.optim.Optimizer\n",
    "    : return: float\n",
    "\n",
    "    Trains the model for one epoch on the training set.\n",
    "    Returns the average accuracy of the epoch.\n",
    "    '''\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    y_preds = torch.tensor([]).to(device)\n",
    "    y_trues = torch.tensor([]).to(device)\n",
    "    model.train()\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_preds = torch.cat((y_preds, y_pred), 0)\n",
    "        y_trues = torch.cat((y_trues, y), 0)\n",
    "    return (y_preds.argmax(1) == y_trues).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, max_epochs = 100, stop_acc = 0.99, seed = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    converged = False\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        train_acc = train_step(model, train_loader, criterion, optimizer)\n",
    "        if stop_acc is not None and train_acc > stop_acc:\n",
    "            converged = True\n",
    "            break\n",
    "    if not converged: print('Convergence not reached, increase epochs')\n",
    "    end_time = time.time()\n",
    "    test_acc = eval_on_dataloader(model, test_loader)\n",
    "    return {'test_acc': test_acc, 'train_time': end_time - start_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_up (seed : int,\n",
    "            batch_size : int = 128,\n",
    "            max_epoch_warm_up : int = 350,\n",
    "            log_interval : int = 5,\n",
    "            Optimizer : torch.optim.Optimizer = torch.optim.SGD,\n",
    "            lr : float = 0.001,\n",
    "            dir : str = 'results'):\n",
    "        \n",
    "    '''\n",
    "    : param seed: int random seed\n",
    "    : param batch_size: int\n",
    "    : param max_epoch_warm_up: int\n",
    "    : param log_interval: int \n",
    "    : param Optimizer: torch.optim.Optimizer\n",
    "    : param lr: float\n",
    "    : param dir: directory to save results\n",
    "    : return: None\n",
    "\n",
    "    Trains the model on half of the dataset for max_epoch_warm_up epochs.\n",
    "    Saves the model every log_interval epochs.\n",
    "    '''\n",
    "\n",
    "    seed_path = f'{dir}/seed{seed}'\n",
    "    checkpoints_path = f'{seed_path}/checkpoints'\n",
    "    # creating directories\n",
    "    os.makedirs(seed_path, exist_ok = True)\n",
    "    os.makedirs(checkpoints_path, exist_ok = True)\n",
    "\n",
    "    # setting seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # getting data loaders\n",
    "    loaders_half = get_cifar10_loaders(0.5, seed = seed, batch_size= batch_size) # half dataset\n",
    "    train_loader_half, test_loader_half = loaders_half['train_loader'], loaders_half['test_loader']\n",
    "\n",
    "    # setting up model training\n",
    "    model = resnet18(num_classes = 10)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = Optimizer(model.parameters(), lr = lr)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    progress = trange(max_epoch_warm_up, position=0)\n",
    "    for epoch in progress:\n",
    "        # training step on half dataset\n",
    "        progress.set_description(f\"Epoch {epoch+1} of {max_epoch_warm_up} (warm up)\")\n",
    "        train_step(model, train_loader_half, criterion, optimizer)\n",
    "        if (epoch+1) % log_interval == 0:\n",
    "            # saving the model\n",
    "            progress.set_description(f\"Epoch {epoch+1} of {max_epoch_warm_up} (saving the model)\")\n",
    "            torch.save(model.state_dict(), f'{checkpoints_path}/warm_up_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_NC(seed: int, dir: str = 'results'):\n",
    "    '''\n",
    "    : param seed: int random seed\n",
    "    : param dir: directory to save results\n",
    "    : return: None\n",
    "\n",
    "    Measures the Neural Collapse of the model trained on half of the dataset.\n",
    "    Saves the results in a pickle file.\n",
    "    '''\n",
    "\n",
    "    seed_path = f'{dir}/seed{seed}'\n",
    "    checkpoints_path = f'{seed_path}/checkpoints'\n",
    "\n",
    "    # setting seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # getting data loaders\n",
    "    loaders_half = get_cifar10_loaders(0.5, seed = seed, batch_size= 128) # half dataset\n",
    "    train_loader_half, test_loader_half = loaders_half['train_loader'], loaders_half['test_loader']\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for epoch in trange(log_interval, max_epoch_warm_up + 1, log_interval):\n",
    "        results.append({})\n",
    "        # setting up model\n",
    "        model = resnet18(num_classes = 10)\n",
    "\n",
    "        # loading the model\n",
    "        model.load_state_dict(torch.load(f'{checkpoints_path}/warm_up_{epoch}.pt'))\n",
    "        model = model.to(device)\n",
    "\n",
    "        # measuring the neural collapse\n",
    "        nc = NC(model, train_loader_half)\n",
    "        results[-1].update(nc)\n",
    "\n",
    "        # saving results\n",
    "        with open(f'{seed_path}/results.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [43:28<00:00, 37.26s/it]\n"
     ]
    }
   ],
   "source": [
    "measure_NC(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_PL(seed: int,\n",
    "               batch_size: int = 128,\n",
    "               max_epoch_full: int = 100,\n",
    "               stop_acc: float = 0.99,\n",
    "               dir : str = 'results'):\n",
    "    '''\n",
    "    : param seed: int random seed\n",
    "    : param batch_size: int\n",
    "    : param max_epoch_full: int maximum number of epochs to train the model on the full dataset\n",
    "    : param stop_acc: float accuracy on the training set to stop training\n",
    "    : param dir: directory to save results\n",
    "    : return: None\n",
    "\n",
    "    Measures the Performance Loss of the model trained on the full dataset.\n",
    "    Saves the results in a pickle file.\n",
    "    '''\n",
    "\n",
    "    seed_path = f'{dir}/seed{seed}'\n",
    "    checkpoints_path = f'{seed_path}/checkpoints'\n",
    "\n",
    "    # load results\n",
    "    with open(f'{seed_path}/results.pkl', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    # setting seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # getting data loaders\n",
    "    loaders_full = get_cifar10_loaders(seed = seed, batch_size= batch_size) # full dataset\n",
    "    train_loader_full, test_loader_full = loaders_full['train_loader'], loaders_full['test_loader']\n",
    "\n",
    "    for epoch in trange(log_interval, max_epoch_warm_up + 1, log_interval):\n",
    "        pos_res = int((epoch / 5) -1)\n",
    "\n",
    "        # setting up model\n",
    "        model = resnet18(num_classes = 10)\n",
    "\n",
    "        # loading the model\n",
    "        model.load_state_dict(torch.load(f'{checkpoints_path}/warm_up_{epoch}.pt'))\n",
    "        model = model.to(device)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "        \n",
    "        # measuring the performance loss\n",
    "        results[pos_res].update(train_and_test(model, train_loader_full, test_loader_full, criterion= criterion, optimizer= optimizer, seed = seed))\n",
    "\n",
    "        if epoch == 5: print(results[pos_res])\n",
    "\n",
    "        # saving results\n",
    "        with open(f'{seed_path}/results.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/70 [10:09<11:40:47, 609.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NC1': 15.429203987121582, 'NC2': 0.37371334433555603, 'NC3': 0.9818062782287598, 'NC4': 0.54756, 'test_acc': 0.607693829113924, 'train_time': 607.2459115982056}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [4:33:52<00:00, 234.75s/it]  \n"
     ]
    }
   ],
   "source": [
    "measure_PL(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SP_NC_PL(seed: int,\n",
    "            batch_size= 128,\n",
    "            dir: str = 'results',\n",
    "            shrink: float = 0.6,\n",
    "            perturb: float = 0.01,\n",
    "            max_epoch_full=100,\n",
    "            stop_acc=0.99,\n",
    "            max_epochs_warm_up = 350):\n",
    "    '''\n",
    "    : param seed: int random seed\n",
    "    : param batch_size: int\n",
    "    : param dir: directory to save results\n",
    "    : param shrink: float\n",
    "    : param perturb: float\n",
    "    : param max_epoch_full: int maximum number of epochs to train the model on the full dataset\n",
    "    : param stop_acc: float accuracy on the training set to stop training\n",
    "    : param max_epochs_warm_up: int maximum number of epochs to train the model on the half dataset\n",
    "    : return: None\n",
    "\n",
    "    Measures the Neural Collapse and the Performance Loss of the model trained on the full dataset after Shrink and Perturb.\n",
    "    Saves the results in a pickle file.\n",
    "    '''\n",
    "\n",
    "    seed_path = f'{dir}/seed{seed}'\n",
    "    checkpoints_path = f'{seed_path}/checkpoints'\n",
    "    # load results\n",
    "    with open(f'{seed_path}/results.pkl', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "     # getting the loaders\n",
    "    loaders_half = get_cifar10_loaders(0.5, seed = seed, batch_size= batch_size) # half dataset\n",
    "    train_loader_half, test_loader_half = loaders_half['train_loader'], loaders_half['test_loader']\n",
    "    loaders_full = get_cifar10_loaders(seed = seed, batch_size= batch_size) # complete dataset\n",
    "    train_loader_full, test_loader_full = loaders_full['train_loader'], loaders_full['test_loader']\n",
    "\n",
    "    progress = trange(5, max_epochs_warm_up+1, 5)\n",
    "    for epoch in progress:\n",
    "        # load model\n",
    "        model = resnet18(num_classes = 10)\n",
    "        model.load_state_dict(torch.load(f'{checkpoints_path}/warm_up_{epoch}.pt'))\n",
    "        dummy_model = resnet18(num_classes = 10)\n",
    "        # shrink and perturb the model\n",
    "        with torch.no_grad():\n",
    "            for real_parameter, random_parameter in zip(model.parameters(), dummy_model.parameters()):\n",
    "                real_parameter.mul_(shrink).add_(random_parameter, alpha=perturb)\n",
    "        Optimizer = torch.optim.SGD\n",
    "        # compute NC\n",
    "        model = model.to(device) # you should change this into nc function\n",
    "        progress.set_description(f\"Epoch {epoch} (measuring NC)\")\n",
    "        nc = NC(model, train_loader_half)\n",
    "        new_key_mapping = {key : f'{key}_SP' for key in nc.keys()}\n",
    "        SP_NC = {new_key_mapping[old_key]: value for old_key, value in nc.items()}\n",
    "        # compute PL\n",
    "        progress.set_description(f\"Epoch {epoch} (computing PL)\")\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = Optimizer(model.parameters(), lr = lr)\n",
    "        \n",
    "        PL = train_and_test(model, train_loader_full, test_loader_full, criterion= criterion, optimizer= optimizer, seed = seed)\n",
    "        new_key_mapping = {key : f'{key}_SP' for key in PL.keys()}\n",
    "        SP_PL = {new_key_mapping[old_key]: value for old_key, value in PL.items()}\n",
    "\n",
    "        pos_res = int((epoch / 5) -1)\n",
    "        results[pos_res].update(SP_NC)\n",
    "        results[pos_res].update(SP_PL)\n",
    "\n",
    "        if epoch == 5: print(results[pos_res])\n",
    "        \n",
    "        with open(f'{seed_path}/results.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 (computing PL):   0%|          | 0/70 [08:30<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NC1': 15.429203987121582, 'NC2': 0.37371334433555603, 'NC3': 0.9818062782287598, 'NC4': 0.54756, 'test_acc': 0.607693829113924, 'train_time': 607.2459115982056, 'NC1_SP': 15.265457153320312, 'NC2_SP': 0.37433314323425293, 'NC3_SP': 0.9828238487243652, 'NC4_SP': 0.55548, 'test_acc_SP': 0.6729628164556962, 'train_time_SP': 470.26064682006836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SP_NC_PL(7) # to do, approx 8 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* Plot as usual, include correlation between the two and have a plot of test accuracy vs nc.\n",
    "* Run the experiments with S&P and measure both NC and PL.\n",
    "* Run experiments with NC regularizer.\n",
    "* Euler (ask Iasonas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NC1</th>\n",
       "      <th>NC2</th>\n",
       "      <th>NC3</th>\n",
       "      <th>NC4</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.429204</td>\n",
       "      <td>0.373713</td>\n",
       "      <td>0.981806</td>\n",
       "      <td>0.54756</td>\n",
       "      <td>0.607694</td>\n",
       "      <td>607.245912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.460551</td>\n",
       "      <td>0.375583</td>\n",
       "      <td>0.966333</td>\n",
       "      <td>0.62512</td>\n",
       "      <td>0.607397</td>\n",
       "      <td>566.765264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.417085</td>\n",
       "      <td>0.370429</td>\n",
       "      <td>0.940527</td>\n",
       "      <td>0.66608</td>\n",
       "      <td>0.596816</td>\n",
       "      <td>534.982836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.435144</td>\n",
       "      <td>0.360533</td>\n",
       "      <td>0.913593</td>\n",
       "      <td>0.69532</td>\n",
       "      <td>0.595134</td>\n",
       "      <td>505.533059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.094497</td>\n",
       "      <td>0.347900</td>\n",
       "      <td>0.882195</td>\n",
       "      <td>0.72988</td>\n",
       "      <td>0.597211</td>\n",
       "      <td>463.956973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.413934</td>\n",
       "      <td>0.202133</td>\n",
       "      <td>0.455983</td>\n",
       "      <td>0.99984</td>\n",
       "      <td>0.571697</td>\n",
       "      <td>168.083418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.411723</td>\n",
       "      <td>0.202033</td>\n",
       "      <td>0.453426</td>\n",
       "      <td>0.99992</td>\n",
       "      <td>0.578026</td>\n",
       "      <td>168.336947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.433576</td>\n",
       "      <td>0.201920</td>\n",
       "      <td>0.456003</td>\n",
       "      <td>0.99964</td>\n",
       "      <td>0.579608</td>\n",
       "      <td>169.117640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.416714</td>\n",
       "      <td>0.201801</td>\n",
       "      <td>0.453658</td>\n",
       "      <td>0.99988</td>\n",
       "      <td>0.578916</td>\n",
       "      <td>168.409798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.409986</td>\n",
       "      <td>0.201687</td>\n",
       "      <td>0.452782</td>\n",
       "      <td>0.99996</td>\n",
       "      <td>0.572884</td>\n",
       "      <td>168.537839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          NC1       NC2       NC3      NC4  test_acc  train_time\n",
       "0   15.429204  0.373713  0.981806  0.54756  0.607694  607.245912\n",
       "1   10.460551  0.375583  0.966333  0.62512  0.607397  566.765264\n",
       "2    7.417085  0.370429  0.940527  0.66608  0.596816  534.982836\n",
       "3    5.435144  0.360533  0.913593  0.69532  0.595134  505.533059\n",
       "4    4.094497  0.347900  0.882195  0.72988  0.597211  463.956973\n",
       "..        ...       ...       ...      ...       ...         ...\n",
       "65   0.413934  0.202133  0.455983  0.99984  0.571697  168.083418\n",
       "66   0.411723  0.202033  0.453426  0.99992  0.578026  168.336947\n",
       "67   0.433576  0.201920  0.456003  0.99964  0.579608  169.117640\n",
       "68   0.416714  0.201801  0.453658  0.99988  0.578916  168.409798\n",
       "69   0.409986  0.201687  0.452782  0.99996  0.572884  168.537839\n",
       "\n",
       "[70 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('results/seed7/results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plnc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
